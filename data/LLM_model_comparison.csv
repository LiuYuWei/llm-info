LLM Model Name,Open Source / Close Source,"MMLU 
 (5-shot)","GPQA 
 (0-shot)","HumanEval 
 (0-shot)","GSM-8K 
 (8-shot, CoT)","MATH 
 (4-shot, CoT)"
Anthropic Claude 3 Opus,Close Source,86.8,"50.4
 (0-shot CoT)",84.9,95,"60.1
 (0-shot CoT)"
Anthropic Claude 3 Sonnet,Close Source,79,"40.4
 (0-shot CoT)",73,92.3,"43.1
 (0-shot CoT)"
Anthropic Claude 3 Haiku,Close Source,75.2,"33.3
 (0-shot CoT)",75.9,88.9,"38.9
 (0-shot CoT)"
Google Gemma 7B,Open Source,53.3,21.4,30.5,30.6,12.2
Google Gemma-2 9B,Open Source,71.3,-,"40.2
(pass@1)","68.6
(5-shot, maj@1)",36.6
Google Gemma-2 27B,Open Source,75.2,-,"51.8
(pass@1)","74.0
(5-shot, maj@1)",42.3
Google Gemini 1.0 Ultra,Close Source,83.7,-,74.4,"94.4
 (Maj1@32)",53.2
Google  Gemini 1.0 Pro,Close Source,71.8,-,67.7,"86.5
 (Maj1@32)",32.6
Google  Gemini 1.5 Pro,Close Source,81.9,"41.5
 (0-shot CoT)",71.9,91.7,58.5
Meta Llama 3 8B,Open Source,68.4,34.2,62.2,79.6,30
Meta Llama 3 70B,Open Source,82,39.5,81.7,93,50.4
Meta Llama 3 400B+,Open Source,86.1,48,84.1,94.1,57.8
Meta Llama 2 7B,Open Source,34.1,21.7,7.9,25.7,3.8
Meta Llama 2 70B,Open Source,52.9,21,25.6,57.5,11.6
MistralAI Mistral 7B,Open Source,58.4,26.3,36.6,39.9,11
MistralAI Mistral large,Close Source,81.2,-,45.1,-,-
OpenAI GPT-4o,Close Source,-,-,-,-,-
OpenAI GPT-4o mini,Close Source,-,-,-,-,-
OpenAI GPT-4,Close Source,86.4,"35.7
 (0-shot CoT)",67,"92.0
 (5-shot CoT)",52.9
OpenAI GPT-3.5,Close Source,70,"28.1
 (0-shot CoT)",48.1,"57.1
 (5-shot)",34.1